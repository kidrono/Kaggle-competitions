{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAZ8RDsulkpHEj30Vlwvba",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kidrono/Kaggle-competitions/blob/main/kaggle_disaster_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle competition: Natural Language Processing with Disaster Tweets\n",
        "The competition information could be found [here](https://www.kaggle.com/competitions/nlp-getting-started/overview)"
      ],
      "metadata": {
        "id": "1JO2P67hZnPp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C24JXicUxQr",
        "outputId": "a6297d15-30a6-4617-ab0a-779a653774f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "f46Tzr_XV_ux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac8c9c7-33aa-43b2-a9d5-fb3ff0bc160b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "Xd-BDNwsWFPq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "O-XI6_nzWG3y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download nlp-getting-started"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-3bSGUMWK-R",
        "outputId": "6cad338f-5344-4243-de3d-d2ffdd528955"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading nlp-getting-started.zip to /content\n",
            "\r  0% 0.00/593k [00:00<?, ?B/s]\n",
            "\r100% 593k/593k [00:00<00:00, 145MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "if os.path.isfile(\"/data/train.csv\"):\n",
        "  print(\"allready exists\")\n",
        "else:\n",
        "  zip_ref = zipfile.ZipFile(\"nlp-getting-started.zip\", 'r')\n",
        "  zip_ref.extractall(\"/data\")\n",
        "  zip_ref.close()"
      ],
      "metadata": {
        "id": "gV26R-AKWv6i"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch"
      ],
      "metadata": {
        "id": "A2siS3npY4n5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/data/train.csv\")\n",
        "train_labels = train_df['target']"
      ],
      "metadata": {
        "id": "0cqcQOAYyMCK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_dict = dict()\n",
        "i = 0\n",
        "for text in train_df['text']:\n",
        "  clean_tweet = re.sub(r'[^a-zA-Z ]+', '', text)\n",
        "  tweet = clean_tweet.split(' ')\n",
        "  for word in tweet:\n",
        "    word = word.lower()\n",
        "    if word not in one_hot_dict.keys():\n",
        "      one_hot_dict[word] = i\n",
        "      i += 1\n",
        "vec_dim = len(one_hot_dict) + 1\n",
        "vec_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGDl-YOnvVF7",
        "outputId": "b947e908-0db4-4d17-b3c5-565dc1f8e6ed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21675"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_vector(text):\n",
        "  vec = np.zeros(vec_dim)\n",
        "  clean_tweet = re.sub(r'[^a-zA-Z ]+', '', text)\n",
        "  tweet = clean_tweet.split(' ')\n",
        "  for word in tweet:\n",
        "    word = word.lower()\n",
        "    if word in one_hot_dict.keys():\n",
        "      vec[one_hot_dict[word]] += 1\n",
        "    else:\n",
        "      vec[-1] += 1\n",
        "  return vec"
      ],
      "metadata": {
        "id": "lX9eiSCzx-Qt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "for text in train_df['text']:\n",
        "  X.append(sentence_vector(text))\n",
        "X = np.array(X)\n",
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ1fTNF603hj",
        "outputId": "3d564385-483a-488b-f697-34db7ccbad36"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7613, 21675)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, classes) -> None:\n",
        "    super().__init__()\n",
        "    self.linear = torch.nn.Linear(input_dim, classes)\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.linear(x)\n",
        "    x = self.sigmoid(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "aDdLt70n5-eb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(x, y, model, loss, optimizer, ephocs):\n",
        "  for epoch in epochs:\n",
        "    y_pred = model(x)\n",
        "    pred_loss = loss(y, y_pred)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n"
      ],
      "metadata": {
        "id": "2BdI3PWd9EbI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(penalty=\"l2\").fit(X, train_labels)\n",
        "y_prob= model.predict_proba(X)[:, 1]\n",
        "y_prob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbRpBVjX-QWN",
        "outputId": "47406713-2340-4297-bfa1-45842c71f7d0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.86471583, 0.9605333 , 0.89823862, ..., 0.90763466, 0.95452407,\n",
              "       0.9931758 ])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "W37Smgm3snDL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_list = []\n",
        "i = 0\n",
        "for text in train_df['text']:\n",
        "  clean_tweet = re.sub(r'[^a-zA-Z ]+', '', text)\n",
        "  tweet = clean_tweet.split(' ')\n",
        "  tokens = []\n",
        "  for word in tweet:\n",
        "    word = word.lower()\n",
        "    tokens.append(word)\n",
        "  tokens_list.append(tokens)\n",
        "train_df['tokens'] = tokens_list\n",
        "\n",
        "train_df['tokens']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqXD27uussFS",
        "outputId": "2d54c365-d6e3-46ee-f4e9-18e2a5ba6182"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [our, deeds, are, the, reason, of, this, earth...\n",
              "1           [forest, fire, near, la, ronge, sask, canada]\n",
              "2       [all, residents, asked, to, shelter, in, place...\n",
              "3       [, people, receive, wildfires, evacuation, ord...\n",
              "4       [just, got, sent, this, photo, from, ruby, ala...\n",
              "                              ...                        \n",
              "7608    [two, giant, cranes, holding, a, bridge, colla...\n",
              "7609    [ariaahrary, thetawniest, the, out, of, contro...\n",
              "7610    [m, , utckm, s, of, volcano, hawaii, httptcozd...\n",
              "7611    [police, investigating, after, an, ebike, coll...\n",
              "7612    [the, latest, more, homes, razed, by, northern...\n",
              "Name: tokens, Length: 7613, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeding = Word2Vec(train_df['text'][0].encode('utf-8').split(), min_count = 1,\n",
        "                              vector_size = 300, window = 5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBnXWBNJuRCx",
        "outputId": "019a37b9-ed6e-4813-ab7f-01601051be25"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.word2vec.Word2Vec at 0x7f302a702b00>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['text'][0].encode('utf-8').split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igv2FiFKurwj",
        "outputId": "5465beda-6f6a-4604-dd03-9bffc1bab6a6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b'Our',\n",
              " b'Deeds',\n",
              " b'are',\n",
              " b'the',\n",
              " b'Reason',\n",
              " b'of',\n",
              " b'this',\n",
              " b'#earthquake',\n",
              " b'May',\n",
              " b'ALLAH',\n",
              " b'Forgive',\n",
              " b'us',\n",
              " b'all']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}